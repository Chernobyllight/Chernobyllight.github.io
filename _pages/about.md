---
permalink: /
title: "Welcome to Hongda Liu's Personal Homepage!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am currently in my third year of pursuing a master degree at Sun Yat-sen University (SYSU). My advisors are [Yulan Guo](https://www.yulanguo.cn/) and [Longguang Wang](https://longguangwang.github.io/). I received my B.S. degree from the School of Information and Software Engineering, University of Electronic Science and Technology of China (UESTC) in 2023.

My current research interests include low-level vision, specifically image/video restoration, as well as image/video style transfer. Besides, I have extensive experience in image/video generation and editing.

## :book: Education

* *2023.09 - 2026.06 (expected)*, M.S. in Sun Yat-sen University (SYSU)
* *2019.09 - 2023.06*, B.S. in University of Electronic Science and Technology of China (UESTC)

## :fire: News


## :books: Publications



<div style="display: flex; flex-direction: column; gap: 40px;">
  
  <div style="display: flex; align-items: flex-start; gap: 20px;">
    <img src="/images/samam2.png" alt="SaMam" style="width: 300px; height: 180px;" >
    <div>
      <strong>SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer</strong><br/>
      <span style="color: red;"><em>CVPR 2025 Highlight</em></span><br/>
      <strong>Hongda Liu</strong>, Longguang Wang, Ye Zhang, Ziru Yu, Yulan Guo<br/>
      <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer_CVPR_2025_paper.html">[paper]</a> 
      <a href="https://github.com/Chernobyllight/SaMam">[code]</a>
    </div>
  </div>
  
  <div style="display: flex; align-items: flex-start; gap: 20px;">
    <img src="/images/SaMST.png" alt="SaMST" style="width: 300px; height: 180px;" >
    <div>
      <strong>Pluggable Style Representation Learning for Multi-Style Transfer</strong><br/>
      <span style="color: red;"><em>ACCV 2024</em></span><br/>
      <strong>Hongda Liu</strong>, Longguang Wang, Weijun Guan, Ye Zhang, Yulan Guo<br/>
      <a href="https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Pluggable_Style_Representation_Learning_for_Multi-Style_Transfer_ACCV_2024_paper.html">[paper]</a> 
      <a href="https://github.com/Chernobyllight/SaMST">[code]</a>
    </div>
  </div>

  <div style="display: flex; align-items: flex-start; gap: 20px;">
    <img src="/images/ReDSR.png" alt="ReDSR" style="width: 300px; height: 180px;" >
    <div>
      <strong>Preserving Full Degradation Details for Blind Image Super-Resolution</strong><br/>
      <span style="color: red;"><em>Arxiv 2024</em></span><br/>
      <strong>Hongda Liu</strong>, Longguang Wang, Ye Zhang, Kaiwen Xue, Shunbo Zhou, Yulan Guo<br/>
      <a href="https://arxiv.org/abs/2407.01299">[paper]</a> 
      <a href="https://github.com/Chernobyllight/ReDSR">[code]</a>
    </div>
  </div>


  <div style="display: flex; align-items: flex-start; gap: 20px;">
    <img src="/images/SFLAM.png" alt="ReDSR" style="width: 300px; height: 180px;" >
    <div>
      <strong>Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning</strong><br/>
      <span style="color: red;"><em>Arxiv 2024</em></span><br/>
      Xianke Qiang, <strong>Hongda Liu</strong>, Xinran Zhang, Zheng Chang, Ying-Chang Liang<br/>
      <a href="https://arxiv.org/abs/2504.09114">[paper]</a> 
    </div>
  </div>
  
</div>


## :computer: Internship


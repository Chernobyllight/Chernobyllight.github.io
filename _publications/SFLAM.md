---
title: "Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning"
collection: publications
category: conferences
excerpt: "Xianke Qiang, **Hongda Liu**, Xinran Zhang, Zheng Chang, Ying-Chang Liang<br />**<font color=red>Arxiv 2024</font>**<br/><img src='/images/SFLAM.png'>"
date: 2024-02-01
venue: 'Arxiv'
paperurl: 'https://arxiv.org/abs/2504.09114'
---
Artificial Intelligence Models (LAMs) powered by massive datasets, extensive parameter scales, and extensive computational resources, leading to significant transformations across various industries. Yet, their practical deployment on resource-limited mobile edge devices is hindered by critical challenges such as data privacy, constrained resources, and high  overhead costs. Addressing this gap, this paper proposes a novel framework, named Quantized Split Federated Fine-Tuning Large AI Model (SFLAM). By partitioning the training load between edge devices and servers using a split learning paradigm, SFLAM can facilitate the operation of large models on devices and significantly lowers the memory requirements on edge devices. Additionally, SFLAM incorporates quantization management, power control, and bandwidth allocation strategies to enhance training efficiency while concurrently reducing energy consumption and communication latency. A theoretical analysis exploring the latency-energy trade-off is presented, and the framework's efficacy is validated via comprehensive simulations. The findings indicate that SFLAM achieves superior performance in terms of learning efficiency and scalability compared to conventional methods, thereby providing a valuable approach for enabling advanced AI services in resource-constrained scenarios.
